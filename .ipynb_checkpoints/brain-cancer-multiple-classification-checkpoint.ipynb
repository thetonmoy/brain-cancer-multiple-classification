{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "009c2fea-80a8-4bc5-9549-51b6b7905b2a",
   "metadata": {},
   "source": [
    "**Loading and Preprocessing the Dataset** <br>\n",
    "The following cells import the necessary libraries. It then loads a dataset (DT-BrainCancer.csv) into a pandas DataFrame. The first column of the dataset is dropped because it is redundant. The categorical columns sex, diagnosis, and loc are transformed using one-hot encoding, which creates binary columns representing the presence of each category within those features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "40ac316d-815a-4186-b11d-710e24db04a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ceabdafe-f6c9-4ec1-9888-945543be1fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('DT-BrainCancer.csv')\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9aa91be7-5aca-4c6e-bab3-432caa2ab05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the unnamed column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2321696f-fee8-4aab-a349-0fbd763bc4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['sex', 'diagnosis', 'loc'])\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bbbf5e-c9bf-49bf-b679-49617a4c0bac",
   "metadata": {},
   "source": [
    "**Splitting the Dataset into Training, Validation, and Test Sets** <br>\n",
    "The dataset is split into three parts: training, validation, and test sets. Initially, 30% of the data is set aside as a temporary dataset (temp_data), leaving 70% for training. The temporary data is further split equally into validation and test sets (15% each). The feature set X_* is obtained by dropping the target column (status), and the target variable y_* is set to the status column. The shapes of these datasets are printed to ensure proper splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f03aa89e-f781-4a5b-b9e3-6524d96c7698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61, 10)\n",
      "(61,)\n",
      "(13, 10)\n",
      "(13,)\n",
      "(14, 10)\n",
      "(14,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, temp_data = train_test_split(data, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train, y_train = train_data.drop('status', axis=1), train_data['status']\n",
    "X_val, y_val = val_data.drop('status', axis=1), val_data['status']\n",
    "X_test, y_test = test_data.drop('status', axis=1), test_data['status']\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c33d36-9e93-4fb7-bdeb-ea314477baad",
   "metadata": {},
   "source": [
    "**Defining the calculate_metrics Function** <br>\n",
    "This function is designed to compute multiple evaluation metrics (accuracy, precision, recall, and F1 score) based on the true and predicted labels. It checks if the inputs are DataFrame objects and converts them to NumPy arrays for compatibility. The function computes the confusion matrix, followed by the individual metrics for each class. It calculates macro-averaged metrics (mean of metrics across all classes) and prints the confusion matrix along with detailed precision, recall, and F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9394bd55-dfbe-40db-8c30-3e4853cf4689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    # Check if y_true is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_true, pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()  # Convert y_true to a NumPy array\n",
    "\n",
    "    # Check if y_pred is a pandas DataFrame and convert it to a NumPy array for compatibility\n",
    "    if isinstance(y_pred, pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()  # Convert y_pred to a NumPy array\n",
    "\n",
    "    # Check if y_true is already flattened (1D)\n",
    "    if y_true.ndim > 1:\n",
    "        y_true = y_true.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Check if y_pred is already flattened (1D)\n",
    "    if y_pred.ndim > 1:\n",
    "        y_pred = y_pred.flatten()  # Flatten if it has more than 1 dimension\n",
    "\n",
    "    # Find unique class names in y_true and determine the number of unique classes\n",
    "    class_names = np.unique(y_true)  # Get unique class names from y_true\n",
    "    unique_classes = class_names.size  # Count the number of unique classes\n",
    "\n",
    "    # Initialize a confusion matrix with zeros, sized based on the number of unique classes\n",
    "    confusion_matrix = np.zeros((unique_classes, unique_classes), dtype=int)\n",
    "\n",
    "    # Map class names to indices for easy lookup\n",
    "    class_name_to_index = {class_name: idx for idx, class_name in enumerate(class_names)}\n",
    "\n",
    "    # Count occurrences of actual vs predicted labels\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        # Ensure actual and predicted are scalar values, not arrays\n",
    "        actual = actual.item() if isinstance(actual, np.ndarray) else actual\n",
    "        predicted = predicted.item() if isinstance(predicted, np.ndarray) else predicted\n",
    "        \n",
    "        # Find the index for the actual and predicted class\n",
    "        actual_index = class_name_to_index[actual]\n",
    "        predicted_index = class_name_to_index[predicted]\n",
    "\n",
    "        # Increment the appropriate cell in the confusion matrix\n",
    "        confusion_matrix[actual_index, predicted_index] += 1\n",
    "\n",
    "    # Print the confusion matrix row by row\n",
    "    print(\"\\nConfusion matrix:\")\n",
    "    for row in confusion_matrix:\n",
    "        print(\" \".join(map(str, row)))  # Print each row of the confusion matrix\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Accuracy Calculation\n",
    "    \n",
    "    # Sum of the diagonal elements (correct predictions)\n",
    "    correct_predictions = np.trace(confusion_matrix)  # np.trace() gives the sum of diagonal elements\n",
    "\n",
    "    # Total number of predictions (sum of all elements in the matrix)\n",
    "    total_predictions = np.sum(confusion_matrix)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    print(f\"\\nAccuracy: {accuracy}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "        \n",
    "    # Precision Calculation\n",
    "\n",
    "    def calculate_precision(confusion_matrix, class_names):\n",
    "        precision = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its precision\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Positive (FP) is the sum of the column (excluding the diagonal)\n",
    "            false_positive = np.sum(confusion_matrix[:, i]) - true_positive\n",
    "            \n",
    "            # Precision for the current class\n",
    "            precision[class_name] = true_positive / (true_positive + false_positive) if (true_positive + false_positive) != 0 else 0        \n",
    "        \n",
    "        return precision\n",
    "    \n",
    "    precision = calculate_precision(confusion_matrix, class_names)\n",
    "    print(\"\\nPrecision for each class:\")\n",
    "    for class_name, precision_value in precision.items():\n",
    "        print(f\"{class_name}: {precision_value:}\")\n",
    "\n",
    "    total_precision = sum(precision.values())\n",
    "    print(f\"\\nMacro precision: {total_precision / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "    \n",
    "    # Recall Calculation\n",
    "    \n",
    "    def calculate_recall(confusion_matrix, class_names):\n",
    "        recall = {}\n",
    "        \n",
    "        # Iterate over each class to calculate its recall\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            # True Positive (TP) is the value in the diagonal for that class\n",
    "            true_positive = confusion_matrix[i, i]\n",
    "            \n",
    "            # False Negative (FN) is the sum of the row (excluding the diagonal)\n",
    "            false_negative = np.sum(confusion_matrix[i, :]) - true_positive\n",
    "            \n",
    "            # Recall for the current class\n",
    "            recall[class_name] = true_positive / (true_positive + false_negative) if (true_positive + false_negative) != 0 else 0        \n",
    "        \n",
    "        return recall\n",
    "    \n",
    "    recall = calculate_recall(confusion_matrix, class_names)\n",
    "    print(\"\\nRecall for each class:\")\n",
    "    for class_name, recall_value in recall.items():\n",
    "        print(f\"{class_name}: {recall_value:.4f}\")\n",
    "\n",
    "    total_recall = sum(recall.values())\n",
    "    print(f\"\\nMacro recall: {total_recall / unique_classes}\")\n",
    "\n",
    "    # --- --- --- --- --- ---\n",
    "\n",
    "    # F1 Score Calculation\n",
    "    \n",
    "    def calculate_f1_score(precision, recall):\n",
    "        f1_scores = {}\n",
    "    \n",
    "        # Calculate F1 score for each class\n",
    "        for class_name in precision.keys():\n",
    "            p = precision[class_name]\n",
    "            r = recall[class_name]\n",
    "            \n",
    "            # Calculate F1 score for the class, handling cases where p + r = 0\n",
    "            f1_scores[class_name] = (2 * p * r) / (p + r) if (p + r) != 0 else 0\n",
    "        \n",
    "        return f1_scores\n",
    "    \n",
    "    f1_scores = calculate_f1_score(precision, recall)\n",
    "    print(\"\\nF1 Score for each class:\")\n",
    "    for class_name, f1_value in f1_scores.items():\n",
    "        print(f\"{class_name}: {f1_value:}\")\n",
    "        \n",
    "    # Macro F1 Score Calculation\n",
    "    total_f1 = sum(f1_scores.values())  # Sum of F1 scores for each class\n",
    "    macro_f1 = total_f1 / len(f1_scores)  # Average F1 score across all classes\n",
    "    \n",
    "    print(f\"\\nMacro F1 score: {macro_f1:}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e844429-e71b-41d4-b948-b7ae340f6240",
   "metadata": {},
   "source": [
    "**Training a K-Nearest Neighbors (KNN) Classifier** <br>\n",
    "A K-Nearest Neighbors classifier is initialized with 6 neighbors. The model is trained using the training data (X_train and y_train). Predictions are made for the training, validation, and test sets (X_train, X_val, and X_test). The calculate_metrics function is called to print the performance metrics for each dataset (training, validation, and test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "de68a7c0-ca41-4dce-9c11-74d002367b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "34 1\n",
      "16 10\n",
      "\n",
      "Accuracy: 0.7213114754098361\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.68\n",
      "1: 0.9090909090909091\n",
      "\n",
      "Macro precision: 0.7945454545454546\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.9714\n",
      "1: 0.3846\n",
      "\n",
      "Macro recall: 0.6780219780219781\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8\n",
      "1: 0.5405405405405405\n",
      "\n",
      "Macro F1 score: 0.6702702702702703\n",
      "------------------------------\n",
      "Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "7 1\n",
      "2 3\n",
      "\n",
      "Accuracy: 0.7692307692307693\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.7777777777777778\n",
      "1: 0.75\n",
      "\n",
      "Macro precision: 0.7638888888888888\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.8750\n",
      "1: 0.6000\n",
      "\n",
      "Macro recall: 0.7375\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.823529411764706\n",
      "1: 0.6666666666666665\n",
      "\n",
      "Macro F1 score: 0.7450980392156863\n",
      "------------------------------\n",
      "Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "10 0\n",
      "2 2\n",
      "\n",
      "Accuracy: 0.8571428571428571\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.8333333333333334\n",
      "1: 1.0\n",
      "\n",
      "Macro precision: 0.9166666666666667\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.5000\n",
      "\n",
      "Macro recall: 0.75\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.9090909090909091\n",
      "1: 0.6666666666666666\n",
      "\n",
      "Macro F1 score: 0.7878787878787878\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_knn = knn.predict(X_train)\n",
    "y_val_pred_knn = knn.predict(X_val)\n",
    "y_test_pred_knn = knn.predict(X_test)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(y_train, y_train_pred_knn)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set Metrics:\")\n",
    "calculate_metrics(y_val, y_val_pred_knn)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set Metrics:\")\n",
    "calculate_metrics(y_test, y_test_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddb765d-4b08-4d02-b6da-1d559b7f06b8",
   "metadata": {},
   "source": [
    "**Training a Naive Bayes Classifier** <br>\n",
    "A Multinomial Naive Bayes classifier is trained using the training data. The model predicts the target variable for the training, validation, and test datasets. The performance metrics are calculated and displayed for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cbea778b-4fd8-42a1-9932-788e71ab610e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "27 8\n",
      "11 15\n",
      "\n",
      "Accuracy: 0.6885245901639344\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.7105263157894737\n",
      "1: 0.6521739130434783\n",
      "\n",
      "Macro precision: 0.681350114416476\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.7714\n",
      "1: 0.5769\n",
      "\n",
      "Macro recall: 0.6741758241758242\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.7397260273972601\n",
      "1: 0.6122448979591837\n",
      "\n",
      "Macro F1 score: 0.6759854626782219\n",
      "------------------------------\n",
      "Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "8 0\n",
      "3 2\n",
      "\n",
      "Accuracy: 0.7692307692307693\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.7272727272727273\n",
      "1: 1.0\n",
      "\n",
      "Macro precision: 0.8636363636363636\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.4000\n",
      "\n",
      "Macro recall: 0.7\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8421052631578948\n",
      "1: 0.5714285714285715\n",
      "\n",
      "Macro F1 score: 0.7067669172932332\n",
      "------------------------------\n",
      "Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "9 1\n",
      "2 2\n",
      "\n",
      "Accuracy: 0.7857142857142857\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.8181818181818182\n",
      "1: 0.6666666666666666\n",
      "\n",
      "Macro precision: 0.7424242424242424\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.9000\n",
      "1: 0.5000\n",
      "\n",
      "Macro recall: 0.7\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8571428571428572\n",
      "1: 0.5714285714285715\n",
      "\n",
      "Macro F1 score: 0.7142857142857144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_nb = nb_classifier.predict(X_train)\n",
    "y_val_pred_nb = nb_classifier.predict(X_val)\n",
    "y_test_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(y_train, y_train_pred_nb)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set Metrics:\")\n",
    "calculate_metrics(y_val, y_val_pred_nb)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set Metrics:\")\n",
    "calculate_metrics(y_test, y_test_pred_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008f0e4f-7d26-47ce-b513-69a318c4d62f",
   "metadata": {},
   "source": [
    "**Training a Support Vector Machine (SVM) Classifier** <br>\n",
    "A Support Vector Machine classifier with a linear kernel is initialized and trained on the training data. Predictions are made for the training, validation, and test sets. Performance metrics are printed for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ff8809d6-c175-44e7-b05b-95e2ffe83073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "31 4\n",
      "14 12\n",
      "\n",
      "Accuracy: 0.7049180327868853\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.6888888888888889\n",
      "1: 0.75\n",
      "\n",
      "Macro precision: 0.7194444444444444\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.8857\n",
      "1: 0.4615\n",
      "\n",
      "Macro recall: 0.6736263736263737\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.775\n",
      "1: 0.5714285714285714\n",
      "\n",
      "Macro F1 score: 0.6732142857142858\n",
      "------------------------------\n",
      "Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "8 0\n",
      "2 3\n",
      "\n",
      "Accuracy: 0.8461538461538461\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.8\n",
      "1: 1.0\n",
      "\n",
      "Macro precision: 0.9\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.6000\n",
      "\n",
      "Macro recall: 0.8\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.888888888888889\n",
      "1: 0.7499999999999999\n",
      "\n",
      "Macro F1 score: 0.8194444444444444\n",
      "------------------------------\n",
      "Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "9 1\n",
      "2 2\n",
      "\n",
      "Accuracy: 0.7857142857142857\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.8181818181818182\n",
      "1: 0.6666666666666666\n",
      "\n",
      "Macro precision: 0.7424242424242424\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.9000\n",
      "1: 0.5000\n",
      "\n",
      "Macro recall: 0.7\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8571428571428572\n",
      "1: 0.5714285714285715\n",
      "\n",
      "Macro F1 score: 0.7142857142857144\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_classifier = SVC(kernel='linear', random_state=42)  # 'linear', 'rbf'\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_svm = svm_classifier.predict(X_train)\n",
    "y_val_pred_svm = svm_classifier.predict(X_val)\n",
    "y_test_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Training Set Metrics:\")\n",
    "calculate_metrics(y_train, y_train_pred_svm)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Validation Set Metrics:\")\n",
    "calculate_metrics(y_val, y_val_pred_svm)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"Test Set Metrics:\")\n",
    "calculate_metrics(y_test, y_test_pred_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3262dbe-b789-4375-9428-c44168dff340",
   "metadata": {},
   "source": [
    "**Training a ZeroR Classifier** <br>\n",
    "The ZeroR classifier, a baseline model, predicts the most frequent class for all instances. It is trained on the training set, and predictions are made for all datasets (training, validation, and test). The model's performance is evaluated by computing the metrics for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4d642e65-f6c5-4df0-9436-ad4aa08eb8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "ZeroR Classifier - Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "35 0\n",
      "26 0\n",
      "\n",
      "Accuracy: 0.5737704918032787\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.5737704918032787\n",
      "1: 0\n",
      "\n",
      "Macro precision: 0.28688524590163933\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.0000\n",
      "\n",
      "Macro recall: 0.5\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.7291666666666666\n",
      "1: 0\n",
      "\n",
      "Macro F1 score: 0.3645833333333333\n",
      "------------------------------\n",
      "ZeroR Classifier - Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "8 0\n",
      "5 0\n",
      "\n",
      "Accuracy: 0.6153846153846154\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.6153846153846154\n",
      "1: 0\n",
      "\n",
      "Macro precision: 0.3076923076923077\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.0000\n",
      "\n",
      "Macro recall: 0.5\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.761904761904762\n",
      "1: 0\n",
      "\n",
      "Macro F1 score: 0.380952380952381\n",
      "------------------------------\n",
      "ZeroR Classifier - Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "10 0\n",
      "4 0\n",
      "\n",
      "Accuracy: 0.7142857142857143\n",
      "\n",
      "Precision for each class:\n",
      "0: 0.7142857142857143\n",
      "1: 0\n",
      "\n",
      "Macro precision: 0.35714285714285715\n",
      "\n",
      "Recall for each class:\n",
      "0: 1.0000\n",
      "1: 0.0000\n",
      "\n",
      "Macro recall: 0.5\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0.8333333333333333\n",
      "1: 0\n",
      "\n",
      "Macro F1 score: 0.41666666666666663\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "zero_r_classifier = DummyClassifier(strategy=\"most_frequent\")\n",
    "zero_r_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_zero_r = zero_r_classifier.predict(X_train)\n",
    "y_val_pred_zero_r = zero_r_classifier.predict(X_val)\n",
    "y_test_pred_zero_r = zero_r_classifier.predict(X_test)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"ZeroR Classifier - Training Set Metrics:\")\n",
    "calculate_metrics(y_train, y_train_pred_zero_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"ZeroR Classifier - Validation Set Metrics:\")\n",
    "calculate_metrics(y_val, y_val_pred_zero_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"ZeroR Classifier - Test Set Metrics:\")\n",
    "calculate_metrics(y_test, y_test_pred_zero_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c2471-4b8a-4616-a11b-1d4b8cccbd29",
   "metadata": {},
   "source": [
    "**Training a OneR Classifier** <br>\n",
    "The OneR classifier, a simple rule-based model, is implemented. It selects the most predictive feature and its best value for classification. The model learns the rules from the training data and makes predictions accordingly. Performance metrics are calculated for the training, validation, and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "509bbe98-c649-4c22-8016-f499dd9fd875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "OneR Classifier - Training Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "0 35\n",
      "0 26\n",
      "\n",
      "Accuracy: 0.4262295081967213\n",
      "\n",
      "Precision for each class:\n",
      "0: 0\n",
      "1: 0.4262295081967213\n",
      "\n",
      "Macro precision: 0.21311475409836064\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.0000\n",
      "1: 1.0000\n",
      "\n",
      "Macro recall: 0.5\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0\n",
      "1: 0.5977011494252873\n",
      "\n",
      "Macro F1 score: 0.29885057471264365\n",
      "------------------------------\n",
      "OneR Classifier - Validation Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "0 8\n",
      "0 5\n",
      "\n",
      "Accuracy: 0.38461538461538464\n",
      "\n",
      "Precision for each class:\n",
      "0: 0\n",
      "1: 0.38461538461538464\n",
      "\n",
      "Macro precision: 0.19230769230769232\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.0000\n",
      "1: 1.0000\n",
      "\n",
      "Macro recall: 0.5\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0\n",
      "1: 0.5555555555555556\n",
      "\n",
      "Macro F1 score: 0.2777777777777778\n",
      "------------------------------\n",
      "OneR Classifier - Test Set Metrics:\n",
      "\n",
      "Confusion matrix:\n",
      "0 10\n",
      "0 4\n",
      "\n",
      "Accuracy: 0.2857142857142857\n",
      "\n",
      "Precision for each class:\n",
      "0: 0\n",
      "1: 0.2857142857142857\n",
      "\n",
      "Macro precision: 0.14285714285714285\n",
      "\n",
      "Recall for each class:\n",
      "0: 0.0000\n",
      "1: 1.0000\n",
      "\n",
      "Macro recall: 0.5\n",
      "\n",
      "F1 Score for each class:\n",
      "0: 0\n",
      "1: 0.4444444444444445\n",
      "\n",
      "Macro F1 score: 0.22222222222222224\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class OneRClassifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.rules = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        self.rules = {}\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            feature_values = X[:, feature_index]\n",
    "            rule_accuracy = {}\n",
    "            for value in np.unique(feature_values):\n",
    "                predicted_class = y[feature_values == value].mode()[0]\n",
    "                rule_accuracy[value] = (y[feature_values == value] == predicted_class).mean()\n",
    "            best_value = max(rule_accuracy, key=rule_accuracy.get)\n",
    "            self.rules[feature_index] = best_value\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X = X.to_numpy() if isinstance(X, pd.DataFrame) else X\n",
    "        predictions = []\n",
    "        for row in X:\n",
    "            row_predictions = [self.rules.get(i, None) for i in range(len(row))]\n",
    "            most_frequent_class = max(set(row_predictions), key=row_predictions.count)  # majority vote for the row\n",
    "            predictions.append(most_frequent_class)\n",
    "        return np.array(predictions)\n",
    "\n",
    "one_r_classifier = OneRClassifier()\n",
    "one_r_classifier.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred_one_r = one_r_classifier.predict(X_train)\n",
    "y_val_pred_one_r = one_r_classifier.predict(X_val)\n",
    "y_test_pred_one_r = one_r_classifier.predict(X_test)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"OneR Classifier - Training Set Metrics:\")\n",
    "calculate_metrics(y_train, y_train_pred_one_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"OneR Classifier - Validation Set Metrics:\")\n",
    "calculate_metrics(y_val, y_val_pred_one_r)\n",
    "\n",
    "print(\"------------------------------\")\n",
    "print(\"OneR Classifier - Test Set Metrics:\")\n",
    "calculate_metrics(y_test, y_test_pred_one_r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
